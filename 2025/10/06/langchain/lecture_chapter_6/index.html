<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hoyuo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.25.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":"flat"},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="🎯 이 챕터에서 배울 것 OpenAI API를 사용하지 않고, 내 컴퓨터에서 직접 언어 모델을 실행하는 방법 HuggingFaceHub: Hugging Face에 호스팅된 모델을 API처럼 사용하는 방법 HuggingFacePipeline: Hugging Face의 모델을 로컬 환경에 다운로드하여 실행하는 방법 GPT4All: 데스크톱 환경에 최적화된 오">
<meta property="og:type" content="article">
<meta property="og:title" content="6. OpenAI를 넘어서 - 로컬 LLM과 PrivateGPT">
<meta property="og:url" content="https://hoyuo.github.io/2025/10/06/langchain/lecture_chapter_6/index.html">
<meta property="og:site_name" content="Hoyuo&#39;s Blog">
<meta property="og:description" content="🎯 이 챕터에서 배울 것 OpenAI API를 사용하지 않고, 내 컴퓨터에서 직접 언어 모델을 실행하는 방법 HuggingFaceHub: Hugging Face에 호스팅된 모델을 API처럼 사용하는 방법 HuggingFacePipeline: Hugging Face의 모델을 로컬 환경에 다운로드하여 실행하는 방법 GPT4All: 데스크톱 환경에 최적화된 오">
<meta property="og:locale" content="ko_KR">
<meta property="article:published_time" content="2025-10-05T15:16:00.000Z">
<meta property="article:author" content="Hoyuo">
<meta property="article:tag" content="langchain">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://hoyuo.github.io/2025/10/06/langchain/lecture_chapter_6/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"ko","comments":true,"permalink":"https://hoyuo.github.io/2025/10/06/langchain/lecture_chapter_6/","path":"2025/10/06/langchain/lecture_chapter_6/","title":"6. OpenAI를 넘어서 - 로컬 LLM과 PrivateGPT"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>6. OpenAI를 넘어서 - 로컬 LLM과 PrivateGPT | Hoyuo's Blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.10.1/mermaid.min.js","integrity":"sha256-BmQmdWDS8X2OTbrwELWK366LV6escyWhHHe0XCTU/Hk="}}</script>
  <script src="/js/third-party/tags/mermaid.js" defer></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hoyuo's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Mobile Developer<br/> (Android, iOS, Flutter, RN)</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="검색" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>홈</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>태그<span class="badge">2</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>카테고리<span class="badge">0</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>아카이브<span class="badge">15</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>검색
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="Searching..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          목차
        </li>
        <li class="sidebar-nav-overview">
          흝어보기
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%8E%AF-%EC%9D%B4-%EC%B1%95%ED%84%B0%EC%97%90%EC%84%9C-%EB%B0%B0%EC%9A%B8-%EA%B2%83"><span class="nav-number">1.</span> <span class="nav-text">🎯 이 챕터에서 배울 것</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HuggingFaceHub"><span class="nav-number">2.</span> <span class="nav-text">HuggingFaceHub</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%8E%AF-%EC%9D%B4%EB%B2%88-%EB%8B%A8%EA%B3%84%EC%97%90%EC%84%9C-%EB%B0%B0%EC%9A%B8-%EA%B2%83"><span class="nav-number">2.1.</span> <span class="nav-text">🎯 이번 단계에서 배울 것</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%93%9D-1%EB%8B%A8%EA%B3%84-Hugging-Face-Hub-%EC%97%B0%EB%8F%99%ED%95%98%EA%B8%B0"><span class="nav-number">2.2.</span> <span class="nav-text">📝 1단계: Hugging Face Hub 연동하기</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%F0%9F%94%8D-%EC%BD%94%EB%93%9C-%EC%83%81%EC%84%B8-%EC%84%A4%EB%AA%85"><span class="nav-number">2.2.1.</span> <span class="nav-text">🔍 코드 상세 설명</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%9C%85-%EC%B2%B4%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8A%B8"><span class="nav-number">2.3.</span> <span class="nav-text">✅ 체크리스트</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HuggingFacePipeline"><span class="nav-number">3.</span> <span class="nav-text">HuggingFacePipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%8E%AF-%EC%9D%B4%EB%B2%88-%EB%8B%A8%EA%B3%84%EC%97%90%EC%84%9C-%EB%B0%B0%EC%9A%B8-%EA%B2%83-1"><span class="nav-number">3.1.</span> <span class="nav-text">🎯 이번 단계에서 배울 것</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%93%9D-1%EB%8B%A8%EA%B3%84-%EB%A1%9C%EC%BB%AC-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8%EC%9C%BC%EB%A1%9C-%EB%AA%A8%EB%8D%B8-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B0"><span class="nav-number">3.2.</span> <span class="nav-text">📝 1단계: 로컬 파이프라인으로 모델 실행하기</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%F0%9F%94%8D-%EC%BD%94%EB%93%9C-%EC%83%81%EC%84%B8-%EC%84%A4%EB%AA%85-1"><span class="nav-number">3.2.1.</span> <span class="nav-text">🔍 코드 상세 설명</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%9C%85-%EC%B2%B4%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8A%B8-1"><span class="nav-number">3.3.</span> <span class="nav-text">✅ 체크리스트</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT4All"><span class="nav-number">4.</span> <span class="nav-text">GPT4All</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%8E%AF-%EC%9D%B4%EB%B2%88-%EB%8B%A8%EA%B3%84%EC%97%90%EC%84%9C-%EB%B0%B0%EC%9A%B8-%EA%B2%83-2"><span class="nav-number">4.1.</span> <span class="nav-text">🎯 이번 단계에서 배울 것</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%93%9D-1%EB%8B%A8%EA%B3%84-GPT4All-%EB%AA%A8%EB%8D%B8-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B0"><span class="nav-number">4.2.</span> <span class="nav-text">📝 1단계: GPT4All 모델 실행하기</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%F0%9F%94%8D-%EC%BD%94%EB%93%9C-%EC%83%81%EC%84%B8-%EC%84%A4%EB%AA%85-2"><span class="nav-number">4.2.1.</span> <span class="nav-text">🔍 코드 상세 설명</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%9C%85-%EC%B2%B4%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8A%B8-2"><span class="nav-number">4.3.</span> <span class="nav-text">✅ 체크리스트</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ollama%EC%99%80-PrivateGPT"><span class="nav-number">5.</span> <span class="nav-text">Ollama와 PrivateGPT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%8E%AF-%EC%9D%B4%EB%B2%88-%EB%8B%A8%EA%B3%84%EC%97%90%EC%84%9C-%EB%B0%B0%EC%9A%B8-%EA%B2%83-3"><span class="nav-number">5.1.</span> <span class="nav-text">🎯 이번 단계에서 배울 것</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%F0%9F%93%9D-1%EB%8B%A8%EA%B3%84-DocumentGPT%EB%A5%BC-PrivateGPT%EB%A1%9C-%EC%A0%84%ED%99%98%ED%95%98%EA%B8%B0"><span class="nav-number">5.2.</span> <span class="nav-text">📝 1단계: DocumentGPT를 PrivateGPT로 전환하기</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%F0%9F%94%8D-%EC%BD%94%EB%93%9C-%EC%83%81%EC%84%B8-%EC%84%A4%EB%AA%85-3"><span class="nav-number">5.2.1.</span> <span class="nav-text">🔍 코드 상세 설명</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E2%9C%85-%EC%B2%B4%ED%81%AC%EB%A6%AC%EC%8A%A4%ED%8A%B8-3"><span class="nav-number">5.3.</span> <span class="nav-text">✅ 체크리스트</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Hoyuo</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">포스트</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">태그</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="https://hoyuo.github.io/2025/10/06/langchain/lecture_chapter_6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hoyuo">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoyuo's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="6. OpenAI를 넘어서 - 로컬 LLM과 PrivateGPT | Hoyuo's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          6. OpenAI를 넘어서 - 로컬 LLM과 PrivateGPT
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>

      <time title="Post created: 2025-10-06 00:16" itemprop="dateCreated datePublished" datetime="2025-10-06T00:16:00+09:00">2025-10-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="🎯-이-챕터에서-배울-것"><a href="#🎯-이-챕터에서-배울-것" class="headerlink" title="🎯 이 챕터에서 배울 것"></a>🎯 이 챕터에서 배울 것</h2><ul>
<li>OpenAI API를 사용하지 않고, 내 컴퓨터에서 직접 언어 모델을 실행하는 방법</li>
<li><code>HuggingFaceHub</code>: Hugging Face에 호스팅된 모델을 API처럼 사용하는 방법</li>
<li><code>HuggingFacePipeline</code>: Hugging Face의 모델을 로컬 환경에 다운로드하여 실행하는 방법</li>
<li><code>GPT4All</code>: 데스크톱 환경에 최적화된 오픈소스 모델을 실행하는 방법</li>
<li><code>Ollama</code>: 로컬 LLM을 가장 쉽게 설치하고 실행할 수 있는 도구와 <code>ChatOllama</code> 연동법</li>
<li><code>OllamaEmbeddings</code>를 사용하여 임베딩 과정까지 로컬에서 처리하여 완벽한 PrivateGPT 구축하기</li>
</ul>
<hr>
<h2 id="HuggingFaceHub"><a href="#HuggingFaceHub" class="headerlink" title="HuggingFaceHub"></a>HuggingFaceHub</h2><h3 id="🎯-이번-단계에서-배울-것"><a href="#🎯-이번-단계에서-배울-것" class="headerlink" title="🎯 이번 단계에서 배울 것"></a>🎯 이번 단계에서 배울 것</h3><ul>
<li>Hugging Face Hub에 있는 수많은 오픈소스 모델을 LangChain과 연동하는 방법</li>
<li><code>HuggingFaceHub</code> 클래스를 사용하여 API 형태로 모델을 호출하는 방법</li>
</ul>
<h3 id="📝-1단계-Hugging-Face-Hub-연동하기"><a href="#📝-1단계-Hugging-Face-Hub-연동하기" class="headerlink" title="📝 1단계: Hugging Face Hub 연동하기"></a>📝 1단계: Hugging Face Hub 연동하기</h3><p><strong>전체 코드 (notebook.ipynb):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> HuggingFaceHub</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 모델에 맞는 프롬프트 템플릿 준비 (Mistral 모델 형식)</span></span><br><span class="line">prompt = PromptTemplate.from_template(<span class="string">&quot;[INST]What is the meaning of &#123;word&#125;[/INST]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. HuggingFaceHub LLM 초기화</span></span><br><span class="line">llm = HuggingFaceHub(</span><br><span class="line">    repo_id=<span class="string">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span>, <span class="comment"># 사용할 모델의 저장소 ID</span></span><br><span class="line">    model_kwargs=&#123;</span><br><span class="line">        <span class="string">&quot;max_new_tokens&quot;</span>: <span class="number">250</span>, <span class="comment"># 모델에 전달할 추가 파라미터</span></span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain = prompt | llm</span><br><span class="line">chain.invoke(&#123;<span class="string">&quot;word&quot;</span>: <span class="string">&quot;potato&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="🔍-코드-상세-설명"><a href="#🔍-코드-상세-설명" class="headerlink" title="🔍 코드 상세 설명"></a>🔍 코드 상세 설명</h4><p><strong>1. <code>HuggingFaceHub</code>란?</strong><br>Hugging Face Hub는 수만 개의 오픈소스 AI 모델이 공유되는 거대한 플랫폼입니다. <code>HuggingFaceHub</code> 클래스는 이 플랫폼에 호스팅된 모델들을 마치 API처럼 호출할 수 있게 해주는 LangChain의 구성 요소입니다.</p>
<ul>
<li><strong>왜 사용하는가?</strong>: 내 컴퓨터에 무거운 모델을 직접 설치하지 않고도 다양한 오픈소스 모델을 테스트하고 사용할 수 있습니다.</li>
<li><strong>어떻게 작동하는가?</strong>: <code>HUGGINGFACEHUB_API_TOKEN</code> 환경 변수를 설정해야 합니다. <code>HuggingFaceHub</code>는 이 토큰을 사용하여 Hugging Face의 Inference API에 요청을 보내고 결과를 받아옵니다.</li>
<li><strong>주의사항</strong>: 모델마다 입력으로 받는 프롬프트의 형식이 다를 수 있습니다. (예: Mistral 모델은 <code>[INST]...[/INST]</code> 형식을 사용) 각 모델의 문서를 확인하고 <code>PromptTemplate</code>을 맞춰주어야 합니다.</li>
</ul>
<h3 id="✅-체크리스트"><a href="#✅-체크리스트" class="headerlink" title="✅ 체크리스트"></a>✅ 체크리스트</h3><ul>
<li><input disabled="" type="checkbox"> Hugging Face 계정을 만들고 API 토큰을 발급받았나요?</li>
<li><input disabled="" type="checkbox"> <code>HUGGINGFACEHUB_API_TOKEN</code> 환경 변수를 설정했나요?</li>
<li><input disabled="" type="checkbox"> <code>HuggingFaceHub</code>를 초기화하고, 원하는 모델의 <code>repo_id</code>를 지정했나요?</li>
<li><input disabled="" type="checkbox"> 모델에 맞는 형식으로 프롬프트를 작성하여 체인을 실행했나요?</li>
</ul>
<hr>
<h2 id="HuggingFacePipeline"><a href="#HuggingFacePipeline" class="headerlink" title="HuggingFacePipeline"></a>HuggingFacePipeline</h2><h3 id="🎯-이번-단계에서-배울-것-1"><a href="#🎯-이번-단계에서-배울-것-1" class="headerlink" title="🎯 이번 단계에서 배울 것"></a>🎯 이번 단계에서 배울 것</h3><ul>
<li>Hugging Face의 모델을 로컬 컴퓨터에 직접 다운로드하여 실행하는 방법</li>
<li><code>HuggingFacePipeline</code>을 사용하여 로컬 모델을 LangChain과 연동하는 방법</li>
</ul>
<h3 id="📝-1단계-로컬-파이프라인으로-모델-실행하기"><a href="#📝-1단계-로컬-파이프라인으로-모델-실행하기" class="headerlink" title="📝 1단계: 로컬 파이프라인으로 모델 실행하기"></a>📝 1단계: 로컬 파이프라인으로 모델 실행하기</h3><p><strong>전체 코드 (notebook.ipynb):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.llms.huggingface_pipeline <span class="keyword">import</span> HuggingFacePipeline</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate.from_template(<span class="string">&quot;A &#123;word&#125; is a&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># HuggingFacePipeline으로 로컬 모델 로드</span></span><br><span class="line">llm = HuggingFacePipeline.from_model_id(</span><br><span class="line">    model_id=<span class="string">&quot;gpt2&quot;</span>, <span class="comment"># 로컬에 다운로드할 모델 ID</span></span><br><span class="line">    task=<span class="string">&quot;text-generation&quot;</span>, <span class="comment"># 파이프라인의 작업 유형</span></span><br><span class="line">    pipeline_kwargs=&#123;<span class="string">&quot;max_new_tokens&quot;</span>: <span class="number">150</span>&#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain = prompt | llm</span><br><span class="line">chain.invoke(&#123;<span class="string">&quot;word&quot;</span>: <span class="string">&quot;tomato&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="🔍-코드-상세-설명-1"><a href="#🔍-코드-상세-설명-1" class="headerlink" title="🔍 코드 상세 설명"></a>🔍 코드 상세 설명</h4><p><strong>1. <code>HuggingFacePipeline</code>이란?</strong><br><code>HuggingFaceHub</code>가 원격 API를 사용하는 것과 달리, <code>HuggingFacePipeline</code>은 Hugging Face의 <code>transformers</code> 라이브러리를 사용하여 모델을 로컬 컴퓨터에 다운로드하고 직접 실행합니다.</p>
<ul>
<li><strong>왜 사용하는가?</strong>: 인터넷 연결 없이 모델을 사용할 수 있으며, 데이터가 외부로 전송되지 않아 보안에 유리합니다. API 비용이 발생하지 않습니다.</li>
<li><strong>어떻게 작동하는가?</strong>: <code>from_model_id</code>가 호출되면, LangChain은 지정된 <code>model_id</code>의 모델 파일을 Hugging Face Hub에서 다운로드합니다(처음 한 번만). 그 후, 해당 모델을 메모리에 로드하여 추론(text-generation) 파이프라인을 구성합니다.</li>
<li><strong>주의사항</strong>: 모델을 실행하려면 충분한 RAM과 VRAM(GPU 사용 시)이 필요합니다. <code>gpt2</code>와 같은 작은 모델로 시작하는 것이 좋습니다. <code>PyTorch</code>와 같은 딥러닝 라이브러리가 설치되어 있어야 합니다.</li>
</ul>
<h3 id="✅-체크리스트-1"><a href="#✅-체크리스트-1" class="headerlink" title="✅ 체크리스트"></a>✅ 체크리스트</h3><ul>
<li><input disabled="" type="checkbox"> <code>transformers</code>, <code>torch</code> 등 필요한 라이브러리를 설치했나요?</li>
<li><input disabled="" type="checkbox"> <code>HuggingFacePipeline.from_model_id</code>를 사용하여 로컬 파이프라인을 생성했나요?</li>
<li><input disabled="" type="checkbox"> 로컬에서 모델이 실행되고 결과를 반환하는 것을 확인했나요?</li>
</ul>
<hr>
<h2 id="GPT4All"><a href="#GPT4All" class="headerlink" title="GPT4All"></a>GPT4All</h2><h3 id="🎯-이번-단계에서-배울-것-2"><a href="#🎯-이번-단계에서-배울-것-2" class="headerlink" title="🎯 이번 단계에서 배울 것"></a>🎯 이번 단계에서 배울 것</h3><ul>
<li><code>GPT4All</code> 라이브러리를 사용하여 데스크톱에 최적화된 로컬 모델을 실행하는 방법</li>
</ul>
<h3 id="📝-1단계-GPT4All-모델-실행하기"><a href="#📝-1단계-GPT4All-모델-실행하기" class="headerlink" title="📝 1단계: GPT4All 모델 실행하기"></a>📝 1단계: GPT4All 모델 실행하기</h3><p><strong>전체 코드 (notebook.ipynb):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.llms.gpt4all <span class="keyword">import</span> GPT4All</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;You are a helpful assistant that defines words. Define this word: &#123;word&#125;.&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GPT4All 모델 파일의 경로를 지정하여 로드</span></span><br><span class="line">llm = GPT4All(</span><br><span class="line">    model=<span class="string">&quot;./falcon.bin&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain = prompt | llm</span><br><span class="line">chain.invoke(&#123;<span class="string">&quot;word&quot;</span>: <span class="string">&quot;tomato&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="🔍-코드-상세-설명-2"><a href="#🔍-코드-상세-설명-2" class="headerlink" title="🔍 코드 상세 설명"></a>🔍 코드 상세 설명</h4><p><strong>1. <code>GPT4All</code>이란?</strong><br><code>GPT4All</code>은 일반적인 소비자용 CPU에서도 잘 작동하도록 최적화된 오픈소스 모델과 생태계입니다. LangChain의 <code>GPT4All</code> 래퍼(wrapper)를 사용하면, 미리 다운로드한 모델 파일(<code>.bin</code>)을 로드하여 쉽게 로컬 추론을 수행할 수 있습니다.</p>
<ul>
<li><strong>왜 사용하는가?</strong>: 강력한 GPU 없이 일반적인 노트북이나 데스크톱에서도 준수한 성능의 LLM을 실행할 수 있습니다.</li>
<li><strong>어떻게 작동하는가?</strong>: 먼저 GPT4All 웹사이트 등에서 원하는 모델의 <code>.bin</code> 파일을 다운로드해야 합니다. 그 후, <code>GPT4All</code> 클래스를 초기화할 때 <code>model</code> 파라미터에 해당 파일의 경로를 지정해주면 됩니다.</li>
</ul>
<h3 id="✅-체크리스트-2"><a href="#✅-체크리스트-2" class="headerlink" title="✅ 체크리스트"></a>✅ 체크리스트</h3><ul>
<li><input disabled="" type="checkbox"> <code>gpt4all</code> 라이브러리를 설치했나요?</li>
<li><input disabled="" type="checkbox"> <code>.bin</code> 형식의 모델 파일을 다운로드했나요?</li>
<li><input disabled="" type="checkbox"> <code>GPT4All</code> 클래스에 모델 파일 경로를 전달하여 LLM을 초기화했나요?</li>
</ul>
<hr>
<h2 id="Ollama와-PrivateGPT"><a href="#Ollama와-PrivateGPT" class="headerlink" title="Ollama와 PrivateGPT"></a>Ollama와 PrivateGPT</h2><h3 id="🎯-이번-단계에서-배울-것-3"><a href="#🎯-이번-단계에서-배울-것-3" class="headerlink" title="🎯 이번 단계에서 배울 것"></a>🎯 이번 단계에서 배울 것</h3><ul>
<li><code>Ollama</code>를 설치하고 사용하여 로컬 LLM을 가장 쉽게 실행하는 방법</li>
<li><code>ChatOllama</code>와 <code>OllamaEmbeddings</code>를 사용하여 <code>DocumentGPT</code>를 완벽한 <code>PrivateGPT</code>로 전환하는 방법</li>
</ul>
<h3 id="📝-1단계-DocumentGPT를-PrivateGPT로-전환하기"><a href="#📝-1단계-DocumentGPT를-PrivateGPT로-전환하기" class="headerlink" title="📝 1단계: DocumentGPT를 PrivateGPT로 전환하기"></a>📝 1단계: <code>DocumentGPT</code>를 <code>PrivateGPT</code>로 전환하기</h3><p><strong>전체 코드 (pages&#x2F;02_PrivateGPT.py):</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ... (다른 import는 DocumentGPT와 유사)</span></span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> OllamaEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOllama</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. LLM을 ChatOllama로 변경</span></span><br><span class="line">llm = ChatOllama(</span><br><span class="line">    model=<span class="string">&quot;mistral:latest&quot;</span>, <span class="comment"># Ollama에서 실행 중인 모델 이름</span></span><br><span class="line">    temperature=<span class="number">0.1</span>,</span><br><span class="line">    streaming=<span class="literal">True</span>,</span><br><span class="line">    callbacks=[ChatCallbackHandler()],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="meta">@st.cache_data(<span class="params">show_spinner=<span class="string">&quot;Embedding file...&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">embed_file</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="comment"># ... (파일 로드 및 분할 로직)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. Embeddings를 OllamaEmbeddings로 변경</span></span><br><span class="line">    embeddings = OllamaEmbeddings(model=<span class="string">&quot;mistral:latest&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)</span><br><span class="line">    vectorstore = FAISS.from_documents(docs, cached_embeddings)</span><br><span class="line">    retriever = vectorstore.as_retriever()</span><br><span class="line">    <span class="keyword">return</span> retriever</span><br><span class="line"></span><br><span class="line"><span class="comment"># ... (나머지 Streamlit UI 코드는 DocumentGPT와 거의 동일)</span></span><br></pre></td></tr></table></figure>

<h4 id="🔍-코드-상세-설명-3"><a href="#🔍-코드-상세-설명-3" class="headerlink" title="🔍 코드 상세 설명"></a>🔍 코드 상세 설명</h4><p><strong>1. <code>Ollama</code>란?</strong><br><code>Ollama</code>는 Llama 2, Mistral 등 다양한 오픈소스 LLM을 로컬 환경에서 매우 쉽게 다운로드하고, 실행하고, 관리할 수 있게 해주는 도구입니다. <code>ollama run &lt;모델명&gt;</code>과 같은 간단한 명령어로 모델을 실행하면, 해당 모델을 API처럼 호출할 수 있는 서버가 로컬에 열립니다.</p>
<p><strong>2. <code>ChatOllama</code> &amp; <code>OllamaEmbeddings</code></strong><br>LangChain은 Ollama와 완벽하게 통합됩니다.</p>
<ul>
<li><strong><code>ChatOllama</code></strong>: 로컬 Ollama 서버에서 실행 중인 모델을 LangChain의 챗 모델처럼 사용할 수 있게 해줍니다. <code>ChatOpenAI</code>를 <code>ChatOllama</code>로 바꾸고, <code>model</code> 이름만 지정해주면 됩니다.</li>
<li><strong><code>OllamaEmbeddings</code></strong>: 임베딩 과정 또한 로컬 Ollama 모델을 사용하여 수행합니다. <code>OpenAIEmbeddings</code>를 <code>OllamaEmbeddings</code>로 교체하기만 하면 됩니다.</li>
</ul>
<p><strong>3. 완벽한 PrivateGPT</strong><br>LLM(추론)과 임베딩을 모두 로컬에서 실행되는 <code>Ollama</code> 모델로 교체함으로써, 파일 데이터와 사용자 질문이 더 이상 외부(OpenAI) 서버로 전송되지 않습니다. 이로써 인터넷 연결 없이도 작동하고 데이터 프라이버시가 완벽하게 보장되는 <code>PrivateGPT</code>가 완성됩니다.</p>
<h3 id="✅-체크리스트-3"><a href="#✅-체크리스트-3" class="headerlink" title="✅ 체크리스트"></a>✅ 체크리스트</h3><ul>
<li><input disabled="" type="checkbox"> <code>Ollama</code>를 설치하고 <code>ollama run mistral</code> 명령어로 모델을 실행했나요?</li>
<li><input disabled="" type="checkbox"> <code>ChatOpenAI</code>를 <code>ChatOllama</code>로, <code>OpenAIEmbeddings</code>를 <code>OllamaEmbeddings</code>로 코드를 수정했나요?</li>
<li><input disabled="" type="checkbox"> 수정된 <code>PrivateGPT</code> 앱이 외부 API 호출 없이 로컬에서 잘 작동하는 것을 확인했나요?</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/langchain/" rel="tag"><i class="fa fa-tag"></i> langchain</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/10/06/langchain/lecture_chapter_5/" rel="prev" title="5. Streamlit으로 DocumentGPT 앱 만들기">
                  <i class="fa fa-angle-left"></i> 5. Streamlit으로 DocumentGPT 앱 만들기
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/10/06/langchain/lecture_chapter_7/" rel="next" title="7. 구조화된 출력 - QuizGPT 만들기">
                  7. 구조화된 출력 - QuizGPT 만들기 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Hoyuo</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
